{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gustapfp/2025.2-G4-bovenau-protalent/blob/main/src/experiments/jupyter/yolo_trainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Yolo Training Model\n",
        "This Jupyter Notebook its used for training a Yolo Model for Welding Detection.\n",
        "\n",
        "## 1. Introduction\n",
        "\n",
        "This notebook uses [Ultralytics](https://docs.ultralytics.com/) to train YOLO11, YOLOv8, or YOLOv5 object detection models with a custom dataset. At the end of this Colab, you'll have a custom YOLO model that you can run on your PC, phone, or edge device like the Raspberry Pi.\n",
        "\n",
        "<p align=center>\n",
        "<img src=\"https://s3.us-west-1.amazonaws.com/evanjuras.com/img/yolo-model-demo.gif\" height=\"360\"><br>\n",
        "<i>Custom YOLO candy detection model in action!</i>\n",
        "</p>"
      ],
      "metadata": {
        "id": "akZJFQsHlhC6"
      },
      "id": "akZJFQsHlhC6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Verify NVIDIA GPU Availability\n",
        "\n",
        "Make sure you're using a GPU-equipped machine by going to \"Runtime\" -> \"Change runtime type\" in the top menu bar, and then selecting one of the GPU options in the Hardware accelerator section. Click Play on the following code block to verify that the NVIDIA GPU is present and ready for training."
      ],
      "metadata": {
        "id": "mF5mlavvl1Wo"
      },
      "id": "mF5mlavvl1Wo"
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTYxGYJjlzmn",
        "outputId": "6233dc48-8fcb-4e68-ab9f-40d2357a5600"
      },
      "id": "DTYxGYJjlzmn",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Sep 14 14:18:53 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   63C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Upload Images\n",
        "Upload .zip folder into colab instance and them split the images"
      ],
      "metadata": {
        "id": "6bnnr0PLpQRD"
      },
      "id": "6bnnr0PLpQRD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1  Split images into train and validation folders\n",
        "At this point, whether you used Option 1, 2, or 3, you should be able to click the folder icon on the left and see your `archive.zip` file in the list of files. Next, we'll unzip `archive.zip` and create some folders to hold the images. Run the following code block to unzip the data."
      ],
      "metadata": {
        "id": "1HEdnVqNpYLF"
      },
      "id": "1HEdnVqNpYLF"
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/archive.zip -d /content/custom_data"
      ],
      "metadata": {
        "id": "GlDoE_0lpUxv"
      },
      "id": "GlDoE_0lpUxv",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ultralytics requires a particular folder structure to store training data for models. Ultralytics requires a particular folder structure to store training data for models. The root folder is named ‚Äúdata‚Äù. Inside, there are two main folders:\n",
        "\n",
        "*   **Train**: These are the actual images used to train the model. In one epoch of training, every image in the train set is passed into the neural network. The training algorithm adjusts the network weights to fit the data in the images.\n",
        "\n",
        "* **Test**: These images are used only after training is completed, to evaluate the model on unseen data and measure its generalization ability.\n",
        "\n",
        "\n",
        "*   **Validation**: These images are used to check the model's performance at the end of each training epoch.\n",
        "\n",
        "In each of these folders is a ‚Äúimages‚Äù folder and a ‚Äúlabels‚Äù folder, which hold the image files and annotation files respectively."
      ],
      "metadata": {
        "id": "1Pcj9JtlqN5F"
      },
      "id": "1Pcj9JtlqN5F"
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the dataset doesn't have this folder structure already, run this Python script which will automatically create the required folder structure and randomly move 90% of dataset to the \"train\" folder and 10% to the \"validation\" folder. Run the following code block to download and execute the scrpt.\n",
        "\n",
        "*The python script was made by: Evan Juras, [EJ Technology Consultants](https://ejtech.io)*"
      ],
      "metadata": {
        "id": "6mm95pLRqs7V"
      },
      "id": "6mm95pLRqs7V"
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the comment here:\n",
        "# !wget -O /content/train_val_split.py https://raw.githubusercontent.com/EdjeElectronics/Train-and-Deploy-YOLO-Models/refs/heads/main/utils/train_val_split.py\n",
        "\n",
        "# TO DO: Improve robustness of train_val_split.py script so it can handle nested data folders, etc\n",
        "# Remove the comment here:\n",
        "# !python train_val_split.py --datapath=\"/content/custom_data\" --train_pct=0.9"
      ],
      "metadata": {
        "id": "pqXPPlvvqitr"
      },
      "id": "pqXPPlvvqitr",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Install the Requirements\n",
        "Before training or testing YOLO models, you need to install the Ultralytics package and its dependencies. This can be done directly from PyPI:"
      ],
      "metadata": {
        "id": "t6hn8Bl1sJsS"
      },
      "id": "t6hn8Bl1sJsS"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "id": "ocfVQGGbrB3z"
      },
      "id": "ocfVQGGbrB3z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Once installed, you can check if everything is working by importing the package and printing its version:\n",
        "\n",
        "The `ultralytics.checks()` command verifies that your environment (CUDA, PyTorch, and dependencies) is correctly set up for training and inference."
      ],
      "metadata": {
        "id": "unfhAcwEsw7Z"
      },
      "id": "unfhAcwEsw7Z"
    },
    {
      "cell_type": "code",
      "source": [
        "import ultralytics\n",
        "ultralytics.checks()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HaMgJLQIs096",
        "outputId": "1e33d078-333b-4593-cbde-fb01091377f5"
      },
      "id": "HaMgJLQIs096",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.199 üöÄ Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Setup complete ‚úÖ (2 CPUs, 12.7 GB RAM, 39.3/112.6 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Tip: If you are working in Google Colab, the installation will only persist while the runtime is active. You‚Äôll need to run the installation cell each time you restart the runtime.*"
      ],
      "metadata": {
        "id": "jXUB0Nv7s8FM"
      },
      "id": "jXUB0Nv7s8FM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here‚Äôs a polished text you can use in your notebook for this activity:\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Creating the Training Configuration File\n",
        "\n",
        "There‚Äôs one last step before we can run training: we need to create the **Ultralytics training configuration YAML file**.\n",
        "\n",
        "This file tells YOLO:\n",
        "\n",
        "* Where the **train**, **validation**, and **test** datasets are located.\n",
        "* Which **classes** the model should learn.\n",
        "\n",
        "Ultralytics expects a `data.yaml` file with paths to your dataset folders and a list of class names.\n",
        "\n",
        "Run the code block below to automatically generate the `data.yaml` configuration file.\n",
        "\n",
        "Make sure you already have a **label map file** located at:\n",
        "\n",
        "```\n",
        "custom_data/classes.txt\n",
        "```\n",
        "\n",
        "If you used **Label Studio** or one of the provided datasets, this file should already exist.\n",
        "If you assembled the dataset manually, you may need to create `classes.txt` yourself, with one class name per line."
      ],
      "metadata": {
        "id": "2gd96UVvt9nx"
      },
      "id": "2gd96UVvt9nx"
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "\n",
        "# # Path to your dataset and classes file\n",
        "# dataset_path = \"/content/custom_data/The Welding Defect Dataset/The Welding Defect Dataset\"\n",
        "# classes_file = os.path.join(dataset_path, \"classes.txt\")\n",
        "\n",
        "# # Read classes from classes.txt\n",
        "# with open(classes_file, \"r\") as f:\n",
        "#     classes = [line.strip() for line in f.readlines() if line.strip()]\n",
        "\n",
        "# # Build YAML content\n",
        "# yaml_content = f\"\"\"\n",
        "# # Ultralytics YOLO dataset configuration\n",
        "# path: {dataset_path}  # root dataset path\n",
        "\n",
        "# train: {os.path.join(dataset_path, \"train/images\")}\n",
        "# val: {os.path.join(dataset_path, \"valid/images\")}\n",
        "# test: {os.path.join(dataset_path, \"test/images\")}\n",
        "\n",
        "# names:\n",
        "# \"\"\"\n",
        "\n",
        "# for i, cls in enumerate(classes):\n",
        "#     yaml_content += f\"  {i}: {cls}\\n\"\n",
        "\n",
        "# # Save to data.yaml\n",
        "# yaml_file = os.path.join(dataset_path, \"data.yaml\")\n",
        "# with open(yaml_file, \"w\") as f:\n",
        "#     f.write(yaml_content)\n",
        "\n",
        "# print(f\"‚úÖ data.yaml created at: {yaml_file}\")\n",
        "# print(\"--- data.yaml preview ---\")\n",
        "# print(yaml_content)\n"
      ],
      "metadata": {
        "id": "mPd1w-iltBfZ"
      },
      "id": "mPd1w-iltBfZ",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Training the Model"
      ],
      "metadata": {
        "id": "a0-6jhevu6XR"
      },
      "id": "a0-6jhevu6XR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Training Model Size and Architecture\n",
        "\n",
        "The figure bellow compares the performance of different YOLO versions and other object detection models using the COCO benchmark. The vertical axis represents validation accuracy (mAP50‚Äì95), while the horizontal axis shows inference latency (ms/image) on a TensorRT10 FP16 T4 GPU.\n",
        "\n",
        "YOLO11 achieves the best balance between accuracy and speed, outperforming earlier YOLO versions (v5‚Äìv10) as well as alternative models like PP-YOLOE+, DAMO-YOLO, YOLOX, and EfficientDet.\n",
        "\n",
        "Smaller models (e.g., YOLO11n, YOLO11s) offer lower latency and are suitable for real-time applications where speed is critical.\n",
        "\n",
        "Larger models (e.g., YOLO11l, YOLO11x) provide higher accuracy at the cost of increased latency, making them more suitable for offline analysis or when computational resources are sufficient.\n",
        "\n",
        "This trade-off between accuracy and inference speed allows practitioners to select the most appropriate YOLO model variant depending on their project requirements, whether prioritizing real-time performance or detection precision.\n",
        "\n",
        "<p align=center>\n",
        "<img src=\"https://raw.githubusercontent.com/ultralytics/assets/refs/heads/main/yolo/performance-comparison.png\" height=\"360\"><br>\n",
        "<i>Yolo Models supported by Ultralytics</i>\n",
        "</p>\n",
        "\n",
        "### 5.2 Number of epochs (epochs)\n",
        "\n",
        "In machine learning, one ‚Äúepoch‚Äù is one single pass through the full training dataset. Setting the number of epochs dictates how long the model will train for. The best amount of epochs to use depends on the size of the dataset and the model architecture. If your dataset has less than 200 images, a good starting point is 60 epochs. If your dataset has more than 200 images, a good starting point is 40 epochs.\n",
        "\n",
        "### 5.3 Resolution (imgsz)\n",
        "\n",
        "Resolution has a large impact on the speed and accuracy of the model: a lower resolution model will have higher speed but less accuracy. YOLO models are typically trained and inferenced at a 640x640 resolution. However, if you want your model to run faster or know you will be working with low-resolution images, try using a lower resolution like 480x480."
      ],
      "metadata": {
        "id": "SS-OMJALv_IO"
      },
      "id": "SS-OMJALv_IO"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BpAMrWGcu-oN"
      },
      "id": "BpAMrWGcu-oN",
      "execution_count": 8,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}